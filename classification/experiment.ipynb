{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "from medmnist import INFO\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check details of 2 channel medmnist images\n",
    "\n",
    "I would suggest this 2 datasets only for baseline experimenation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "[  0   2   3   4   5   6   8  11  19  21  23  25  27  28  37  42  45  46\n",
      "  49  53  55  57  60  62  64  70  77  79  81  83  84  85  86  87  89  90\n",
      "  91  92  93  94  96  97  98  99 100 101 102 103 104 105 106 107 108 109\n",
      " 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127\n",
      " 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
      " 146 147 148 149 150 151 152 153 154 155 156 158 159 160 161 162 163 164\n",
      " 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218\n",
      " 219 220 221 224 225]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/localssk23/.medmnist/pneumoniamnist.npz\"\n",
    "data = np.load(dataset_path)\n",
    "image = data['train_images'][0]\n",
    "print(image.shape)\n",
    "print(np.unique(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/localssk23/.medmnist/retinamnist.npz\"\n",
    "data = np.load(dataset_path)\n",
    "image = data['train_images'][0]\n",
    "print(image.shape)\n",
    "print(np.unique(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTPerturbedDataset(Dataset):\n",
    "    def __init__(self, images, labels, perturbation_percentage, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Original image dataset\n",
    "            labels: Corresponding labels\n",
    "            perturbation_percentage: Percentage of pixels to perturb (0-100)\n",
    "            transform: Optional transform to be applied on the perturbed image\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.perturbation_percentage = perturbation_percentage\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply percentage-wise perturbation\n",
    "        perturbed_image = self.perturb_image_percentage(image, self.perturbation_percentage)\n",
    "        \n",
    "        if self.transform:\n",
    "            perturbed_image = self.transform(perturbed_image)\n",
    "            \n",
    "        return perturbed_image, label\n",
    "    \n",
    "    def perturb_image_percentage(self, img, percent):\n",
    "        # Calculate number of pixels to perturb\n",
    "        total_pixels = img.shape[0] * img.shape[1]\n",
    "        \n",
    "        if percent is None or percent == 0:\n",
    "            num_pixels = 0\n",
    "        elif percent == 'one': #! Single pixel perturbation\n",
    "            num_pixels = 1\n",
    "        else:\n",
    "            num_pixels = int(total_pixels * percent / 100)\n",
    "        \n",
    "        # Create a copy of the image\n",
    "        perturbed_img = img.copy()\n",
    "        \n",
    "        # Generate random pixel positions\n",
    "        positions = np.random.choice(total_pixels, num_pixels, replace=False)\n",
    "        x_positions = positions // img.shape[1]\n",
    "        y_positions = positions % img.shape[1]\n",
    "        \n",
    "        # Generate random values for selected pixels\n",
    "        if img.ndim == 3:\n",
    "            values = np.random.randint(0, 255, size=(num_pixels, img.shape[2]))\n",
    "        else:\n",
    "            values = np.random.randint(0, 255, size=num_pixels)\n",
    "        \n",
    "        # Modify the pixels\n",
    "        for i in range(num_pixels):\n",
    "            if img.ndim == 3:\n",
    "                perturbed_img[x_positions[i], y_positions[i]] = values[i]\n",
    "            else:\n",
    "                perturbed_img[x_positions[i], y_positions[i]] = values[i]\n",
    "                \n",
    "        return perturbed_img\n",
    "\n",
    "################################################################\n",
    "############# 1 pixel perturbation orig paper code #############\n",
    "################################################################\n",
    "#? Paper: arxiv.org/abs/1710.08864\n",
    "#? Code: https://github.com/Hyperparticle/one-pixel-attack-keras/blob/master/1_one-pixel-attack-cifar10.ipynb\n",
    "\n",
    "# def perturb_image(xs, img):\n",
    "#     # If this function is passed just one perturbation vector,\n",
    "#     # pack it in a list to keep the computation the same\n",
    "#     if xs.ndim < 2:\n",
    "#         xs = np.array([xs])\n",
    "    \n",
    "#     # Copy the image n == len(xs) times so that we can \n",
    "#     # create n new perturbed images\n",
    "#     tile = [len(xs)] + [1]*(xs.ndim+1)\n",
    "#     imgs = np.tile(img, tile)\n",
    "    \n",
    "#     # Make sure to floor the members of xs as int types\n",
    "#     xs = xs.astype(int)\n",
    "    \n",
    "#     height, width = img.shape[:2]\n",
    "    \n",
    "#     for x, img in zip(xs, imgs):\n",
    "#         # Split x into an array of 5-tuples (perturbation pixels)\n",
    "#         # i.e., [[x,y,r,g,b], ...] for RGB or [[x,y,value], ...] for BW\n",
    "#         pixels = np.split(x, len(x) // (2 + CONFIG['num_channels']))\n",
    "#         for pixel in pixels:\n",
    "#             x_pos, y_pos, *values = pixel\n",
    "#             # Add bounds checking\n",
    "#             x_pos = np.clip(x_pos, 0, height - 1)\n",
    "#             y_pos = np.clip(y_pos, 0, width - 1)\n",
    "            \n",
    "#             if CONFIG['num_channels'] == 1:\n",
    "#                 img[x_pos, y_pos] = values[0]\n",
    "#             else:\n",
    "#                 img[x_pos, y_pos] = values\n",
    "    \n",
    "#     return imgs\n",
    "\n",
    "class MNISTGaussianDataset(Dataset):\n",
    "    def __init__(self, images, labels, noise_std, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Original image dataset\n",
    "            labels: Corresponding labels\n",
    "            noise_std: Standard deviation of the Gaussian noise to be added\n",
    "            transform: Optional transform to be applied on the noisy image\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.noise_std = noise_std\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply Gaussian noise\n",
    "        noisy_image = self.add_gaussian_noise(image, self.noise_std)\n",
    "        \n",
    "        if self.transform:\n",
    "            noisy_image = self.transform(noisy_image)\n",
    "            \n",
    "        return noisy_image, label\n",
    "\n",
    "    def add_gaussian_noise(self, image, noise_percent):\n",
    "        # Convert percentage to standard deviation based on image range\n",
    "        noise_std = (noise_percent/100.0) * np.max(image)\n",
    "        \n",
    "        # Generate noise\n",
    "        noise = np.random.normal(0, noise_std, image.shape)\n",
    "        \n",
    "        # Add noise and clip to valid range\n",
    "        noisy_image = np.clip(image + noise, 0, 255)\n",
    "        \n",
    "        return noisy_image.astype(np.uint8)\n",
    "    \n",
    "\n",
    "class MNISTRemoveDataset(Dataset):\n",
    "    def __init__(self, images, labels, perturbation_percentage, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Original image dataset\n",
    "            labels: Corresponding labels\n",
    "            perturbation_percentage: Percentage of pixels to remove (0-100)\n",
    "            transform: Optional transform to be applied on the perturbed image\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.perturbation_percentage = perturbation_percentage\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply percentage-wise perturbation\n",
    "        perturbed_image = self.remove_pixel_percentage(image, self.perturbation_percentage)\n",
    "        \n",
    "        if self.transform:\n",
    "            perturbed_image = self.transform(perturbed_image)\n",
    "            \n",
    "        return perturbed_image, label\n",
    "    \n",
    "    def remove_pixel_percentage(self, img, percent):\n",
    "        # Calculate number of pixels to perturb\n",
    "        total_pixels = img.shape[0] * img.shape[1]\n",
    "        \n",
    "        if percent is None or percent == 0:\n",
    "            num_pixels = 0\n",
    "        elif percent == 'one': #! Single pixel perturbation\n",
    "            num_pixels = 1\n",
    "        else:\n",
    "            num_pixels = int(total_pixels * percent / 100)\n",
    "        \n",
    "        # Create a copy of the image\n",
    "        perturbed_img = img.copy()\n",
    "        \n",
    "        # Generate random pixel positions\n",
    "        positions = np.random.choice(total_pixels, num_pixels, replace=False)\n",
    "        x_positions = positions // img.shape[1]\n",
    "        y_positions = positions % img.shape[1]\n",
    "        \n",
    "        # Set selected pixels to 0\n",
    "        if img.ndim == 3:\n",
    "            values = np.zeros((num_pixels, img.shape[2]), dtype=img.dtype)\n",
    "        else:\n",
    "            values = np.zeros(num_pixels, dtype=img.dtype)\n",
    "        \n",
    "        # Modify the pixels\n",
    "        for i in range(num_pixels):\n",
    "            if img.ndim == 3:\n",
    "                perturbed_img[x_positions[i], y_positions[i]] = values[i]\n",
    "            else:\n",
    "                perturbed_img[x_positions[i], y_positions[i]] = values[i]\n",
    "                \n",
    "        return perturbed_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_28(nn.Module): #? Notice GroupNorm Used.\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Net_28, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3),\n",
    "            nn.GroupNorm(4, 16),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3),\n",
    "            nn.GroupNorm(4, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size=3),\n",
    "            nn.GroupNorm(16, 64),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.GroupNorm(16, 64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(16, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# class Net_224(nn.Module):\n",
    "#     def __init__(self, in_channels, num_classes):\n",
    "#         super(Net_224, self).__init__()\n",
    "\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, 16, kernel_size=3),\n",
    "#             nn.GroupNorm(4, 16),\n",
    "#             nn.ReLU())\n",
    "\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(16, 16, kernel_size=3),\n",
    "#             nn.GroupNorm(4, 16),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "#         self.layer3 = nn.Sequential(\n",
    "#             nn.Conv2d(16, 64, kernel_size=3),\n",
    "#             nn.GroupNorm(16, 64),\n",
    "#             nn.ReLU())\n",
    "\n",
    "#         self.layer4 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, kernel_size=3),\n",
    "#             nn.GroupNorm(16, 64),\n",
    "#             nn.ReLU())\n",
    "\n",
    "#         self.layer5 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "#             nn.GroupNorm(16, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "#         self.layer6 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 128, kernel_size=3),\n",
    "#             nn.GroupNorm(32, 128),\n",
    "#             nn.ReLU())\n",
    "\n",
    "#         self.layer7 = nn.Sequential(\n",
    "#             nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "#             nn.GroupNorm(32, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(128 * 25 * 25, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, num_classes))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "#         x = self.layer5(x)\n",
    "#         x = self.layer6(x)\n",
    "#         x = self.layer7(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(all_targets, all_outputs, task):\n",
    "    all_targets, all_outputs = np.array(all_targets), np.array(all_outputs)\n",
    "    if task == 'multi-label, binary-class':\n",
    "        return roc_auc_score(all_targets, all_outputs, average='macro')\n",
    "    elif all_outputs.shape[1] == 2:\n",
    "        return roc_auc_score(all_targets, all_outputs[:, 1])\n",
    "    else:\n",
    "        softmax_outputs = softmax(all_outputs, axis=1)\n",
    "        return roc_auc_score(all_targets, softmax_outputs, multi_class='ovr', average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = '/home/localssk23/'\n",
    "\n",
    "dataset = 'retinamnist'\n",
    "\n",
    "CONFIG = {\n",
    "   \"batch_size\": 2,\n",
    "   \"num_epochs\": 1,\n",
    "\n",
    "   \"device\": torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "   \n",
    "   \"data_path\": f'{HOME}.medmnist/{dataset}.npz',\n",
    "   \"result_path\": f'{HOME}localdp/classification/results/',\n",
    "\n",
    "   \"num_folds\": 1\n",
    "}\n",
    "\n",
    "DATASET = {\n",
    "    'Perturbed': MNISTPerturbedDataset,\n",
    "    'Removal': MNISTRemoveDataset,\n",
    "    'Gaussian': MNISTGaussianDataset\n",
    "}\n",
    "\n",
    "PERTURBATION_VALUES = {\n",
    "        'original': None,\n",
    "        'noisy_1pixel': 'one',\n",
    "        'noisy_1%': 1,\n",
    "        'noisy_5%': 5,\n",
    "        'noisy_10%': 10,\n",
    "        'noisy_25%': 25,\n",
    "        'noisy_50%': 50,\n",
    "        'noisy_75%': 75,\n",
    "        'noisy_90%': 90,\n",
    "        'noisy_99%': 99\n",
    "    }\n",
    "\n",
    "REMOVAL_VALUES = {\n",
    "        'original': None,\n",
    "        'remove_1pixel': 'one',\n",
    "        'remove_1%': 1,\n",
    "        'remove_5%': 5,\n",
    "        'remove_10%': 10,\n",
    "        'remove_25%': 25,\n",
    "        'remove_50%': 50,\n",
    "        'remove_75%': 75,\n",
    "        'remove_90%': 90,\n",
    "        'remove_99%': 99\n",
    "    }\n",
    "\n",
    "GAUSSIAN_STD_VALUES = {\n",
    "    'gaussian_1': 1,\n",
    "    'gaussian_5': 5,\n",
    "    'gaussian_10': 10,\n",
    "    'gaussian_25': 25,\n",
    "    'gaussian_50': 50,\n",
    "    'gaussian_75': 75,\n",
    "    'gaussian_90': 90,\n",
    "    'gaussian_99': 99\n",
    "    }\n",
    "\n",
    "data_flag = CONFIG['data_path'].split('/')[-1].split('.')[0]\n",
    "info = INFO[data_flag]\n",
    "\n",
    "device = CONFIG['device']\n",
    "\n",
    "CONFIG['num_classes'] = len(info['label'])\n",
    "CONFIG['num_channels'] = info['n_channels']\n",
    "CONFIG['task'] = info['task']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(epoch):\n",
    "    initial_lr = 0.001  # Initial learning rate\n",
    "    if epoch < 50:\n",
    "        return initial_lr / initial_lr  # Learning rate remains 0.001\n",
    "    elif epoch < 75:\n",
    "        return 0.1 * initial_lr / initial_lr  # Delay learning rate to 0.0001 after 50 epochs\n",
    "    else:\n",
    "        return 0.01 * initial_lr / initial_lr  # Delay learning rate to 0.00001 after 75 epochs\n",
    "\n",
    "def train(model, train_loader, task):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "\n",
    "    if task == \"multi-label, binary-class\":\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss() #! Change to MSE (or just add it)\n",
    "    criterion.to(device)\n",
    "\n",
    "    lr = 0.001\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(CONFIG['num_epochs'])):\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.float().to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if task == 'multi-label, binary-class':\n",
    "                targets = targets.to(torch.float32)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Calculate accuracy for multi-label\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                correct += (predicted == targets).all(dim=1).sum().item()\n",
    "            else:\n",
    "                targets = targets.squeeze().long()\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Calculate accuracy for standard classification\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            total += targets.size(0)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store targets and outputs for AUC calculation\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_outputs.extend(outputs.detach().cpu().numpy())\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, task):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    class_correct = {}\n",
    "    class_total = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.float().to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if task == 'multi-label, binary-class':\n",
    "                targets = targets.to(float().torch.float32)\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                correct += (predicted == targets).all(dim=1).sum().item()\n",
    "            else:\n",
    "                targets = targets.squeeze().long()\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "                \n",
    "                # Calculate class-wise accuracy\n",
    "                for true, pred in zip(targets, predicted):\n",
    "                    if true.ndim == 0:\n",
    "                        true = int(true.item())\n",
    "                        pred = int(pred.item())\n",
    "                    else:\n",
    "                        true = tuple(true.cpu().numpy())\n",
    "                        pred = tuple(pred.cpu().numpy())\n",
    "                    \n",
    "                    if true == pred:\n",
    "                        class_correct[true] = class_correct.get(true, 0) + 1\n",
    "                    class_total[true] = class_total.get(true, 0) + 1\n",
    "\n",
    "            total += targets.size(0)\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    class_acc = {k: 100. * v / class_total[k] for k, v in class_correct.items()}\n",
    "\n",
    "    auc = compute_auc(all_targets, all_outputs, task)\n",
    "    \n",
    "    return acc, auc, class_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelResults:\n",
    "    accuracy: float\n",
    "    auc: float\n",
    "    class_acc: Dict[int, float]\n",
    "\n",
    "def create_data_transform(num_channels: int) -> transforms.Compose:\n",
    "    stats = [0.5] * num_channels\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=stats, std=stats)\n",
    "    ])\n",
    "\n",
    "def setup_datasets(\n",
    "    data: Dict,\n",
    "    dataset_class: type,\n",
    "    loader_type: str, \n",
    "    transform,\n",
    "    batch_size: int = 20,\n",
    "    shuffle: bool = False\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Creates train and test datasets using the provided dataset class.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary containing train and test data\n",
    "        dataset_class: The dataset class to instantiate\n",
    "        loader_type: Type of data loader to use\n",
    "        transform: Transformations to apply to the data\n",
    "        batch_size: Batch size for the DataLoader\n",
    "        shuffle: Whether to shuffle the data\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_dataloader, test_dataloader)\n",
    "    \"\"\"\n",
    "    train_dataset = dataset_class(\n",
    "        data['train_images'],\n",
    "        data['train_labels'],\n",
    "        loader_type,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    test_dataset = dataset_class(\n",
    "        data['test_images'],\n",
    "        data['test_labels'],\n",
    "        loader_type,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle),\n",
    "        DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: retinamnist\n",
      "FOLD: 0\n",
      "DATASET_STYLE: Perturbed\n",
      "Loader Type: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_STYLE: Removal\n",
      "Loader Type: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_STYLE: Gaussian\n",
      "Loader Type: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader Type: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(loaders_dict: Dict[str, Dict], dataset_style) -> Dict[str, ModelResults]:\n",
    "    results = {}\n",
    "    \n",
    "    for loader_type in loaders_dict.values():\n",
    "        print(f'Loader Type: {loader_type}')\n",
    "        \n",
    "        # Load and prepare data\n",
    "        data = np.load(CONFIG['data_path'])\n",
    "        transform = create_data_transform(CONFIG['num_channels'])\n",
    "        \n",
    "        # Setup model and datasets\n",
    "        model = Net_28(CONFIG['num_channels'], CONFIG['num_classes']).to(CONFIG['device'])\n",
    "        train_loader, test_loader = setup_datasets(data, DATASET[dataset_style], loader_type, transform)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        model_trained = train(model, train_loader, CONFIG['task'])\n",
    "        acc, auc, class_acc = test(model_trained, test_loader, CONFIG['task'])\n",
    "        \n",
    "        results[loader_type] = ModelResults(\n",
    "            accuracy=acc,\n",
    "            auc=auc,\n",
    "            class_acc=class_acc\n",
    "        )\n",
    "        \n",
    "        del model, model_trained\n",
    "            \n",
    "    return results\n",
    "\n",
    "def process_results(results: Dict, dataset_name: str, fold: int, dataset_style: str):\n",
    "    class_results = []\n",
    "    overall_results = []\n",
    "    \n",
    "    # Process class-wise results\n",
    "    for label in range(CONFIG['num_classes']):\n",
    "        result_dict = {'Dataset': dataset_name, 'Class': label}\n",
    "        for loader_type, metrics in results.items():\n",
    "            result_dict[f'Accuracy_{loader_type}'] = metrics.class_acc.get(label, 0)\n",
    "        class_results.append(result_dict)\n",
    "    \n",
    "    # Process overall results\n",
    "    overall_dict = {'Dataset': dataset_name}\n",
    "    for loader_type, metrics in results.items():\n",
    "        overall_dict[f'Overall_Accuracy_{loader_type}'] = metrics.accuracy\n",
    "        overall_dict[f'Overall_AUC_{loader_type}'] = metrics.auc\n",
    "    overall_results.append(overall_dict)\n",
    "\n",
    "    # # Save results\n",
    "    # pd.DataFrame(class_results).to_csv(\n",
    "    #     f\"{CONFIG['result_path']}{dataset_name}_class_results_fold_{fold}_{dataset_style}.csv\",\n",
    "    #     index=False\n",
    "    # )\n",
    "    # pd.DataFrame(overall_results).to_csv(\n",
    "    #     f\"{CONFIG['result_path']}{dataset_name}_overall_results_fold_{fold}_{dataset_style}.csv\",\n",
    "    #     index=False\n",
    "    # )\n",
    "\n",
    "def main(fold: int):\n",
    "    dataset_name = CONFIG['data_path'].rsplit('/', 1)[-1].rsplit('.', 1)[0]\n",
    "    print(f'DATA: {dataset_name}')\n",
    "    print(f'FOLD: {fold}')\n",
    "\n",
    "    for dataset_style in DATASET.keys():\n",
    "        print(f'DATASET_STYLE: {dataset_style}')\n",
    "        if dataset_style == 'Gaussian':\n",
    "            results = train_and_evaluate(GAUSSIAN_STD_VALUES, dataset_style)\n",
    "            process_results(results, dataset_name, fold, dataset_style)\n",
    "        elif dataset_style == 'Perturbed':\n",
    "            results = train_and_evaluate(PERTURBATION_VALUES, dataset_style)\n",
    "            process_results(results, dataset_name, fold, dataset_style)\n",
    "        elif dataset_style == 'Removal':\n",
    "            results = train_and_evaluate(REMOVAL_VALUES, dataset_style)\n",
    "            process_results(results, dataset_name, fold, dataset_style)\n",
    "    print()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for fold in range(CONFIG['num_folds']):\n",
    "        main(fold)\n",
    "        print()\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cucim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
